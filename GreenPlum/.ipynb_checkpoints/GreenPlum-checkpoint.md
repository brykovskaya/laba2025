# üçà GreenPlum
---
## üçà –£—Å—Ç–∞–Ω–æ–≤–∫–∞ GreenPlum

–ó–∞–ø—É—Å–∫–∞–µ–º docker-compose file:
![](images/4.png)<br>
![](images/5.png)<br>
```
C:\Users\79181>mkdir docker-greenplum
C:\Users\79181>cd docker-greenplum
C:\Users\79181\docker-greenplum>docker-compose up -d
```
–°–æ–∑–¥–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã:<br>
![](images/1.png)<br>


---

## üçà –£–ø—Ä. GreenPlum - –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö –∏–∑ CSV-—Ñ–∞–π–ª–∞
–°–∫–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª [sales_data.csv](./files/sales_data.csv):
```
C:\Users\79181\docker-greenplum>docker exec -it db_master sudo mkdir -p /data
C:\Users\79181\docker-greenplum>docker cp sales_data.csv db_master:/data/

–¢–µ–ø–µ—Ä—å –∑–∞–π–¥–µ–º db_master –∏ –ø–æ—Å–º–æ—Ä—Ç–∏–º —á—Ç–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–ª–æ—Å—å –≤ /data/

C:\Users\79181\docker-greenplum>docker exec -it db_master /bin/bash
[root@db_master /]# cd /data/
[root@db_master data]# ls
sales_data.csv
```
![](images/6.png)<br>
![](images/2.png)<br>
–ü—Ä–æ–±—Ä–æ—Å–∏–º –ø–æ—Ä—Ç, —á—Ç–æ–±—ã —Ä–∞–±–æ—Ç–∞—Ç—å —á–µ—Ä–µ–∑ gpfdist –Ω–∞ 2 —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:

```
C:\Users\79181\docker-greenplum>docker exec -it db_master /bin/bash
[root@db_master /]# su - gpadmin
[gpadmin@db_master ~]$ gpfdist -d /data -p 5435 &
[1] 1208
[gpadmin@db_master ~]$ 2025-04-13 17:05:54 1208 INFO Before opening listening sockets - following listening sockets are available:
2025-04-13 17:05:55 523 INFO IPV6 socket: [::]:5435
2025-04-13 17:05:55 523 INFO IPV4 socket: 0.0.0.0:5435
2025-04-13 17:05:55 523 INFO Trying to open listening socket:
2025-04-13 17:05:55 523 IPV6 socket: [::]:5435
2025-04-13 17:05:55 523  INFO Opening listening socket succeeded
2025-04-13 17:05:55 523  INFO Trying to open listening socket:
2025-04-13 17:05:55 523 INFO IPV4 socket: 0.0.0.0:5435
Serving HTTP on port 5435, directory /data
```

–í–æ 2 —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∑–∞–π–¥–µ–º –≤ psql:

```
C:\Users\79181>cd docker-greenplum

C:\Users\79181\docker-greenplum>docker exec -it db_master /bin/bash
[root@db_master /]# psql -U gpadmin -d postgres
bash: psql: command not found
[root@db_master /]# /opt/greenplum/bin/psql -U gpadmin -d postgres
psql (9.4.24)
Type "help" for help.

postgres=#
```

–°–æ–∑–¥–∞–¥–∏–º –≤–Ω–µ—à–Ω—é—é —Ç–∞–±–ª–∏—Ü—É, –∑–∞–≥—Ä—É–∑–∏–º –≤ –Ω–µ–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ csv-file, –ø–æ—Å–ª–µ —á–µ–≥–æ –≤—Å—Ç–∞–≤–∏–º –∏—Ö —É–∂–µ –≤ –ø–æ—Å—Ç–æ—è–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É:

```
postgres=# CREATE EXTERNAL TABLE sales_external (
postgres(#   product_id INT,
postgres(#   sales_date DATE,
postgres(#   quantity INT
postgres(# )
postgres-# LOCATION ('gpfdist://172.24.0.5:5435/sales_data.csv')
postgres-# FORMAT 'CSV' (HEADER);
NOTICE:  HEADER means that each one of the data files has a header row
CREATE EXTERNAL TABLE
postgres=# CREATE TABLE sales (LIKE sales_external) DISTRIBUTED BY (product_id);
CREATE TABLE
postgres=# INSERT INTO sales SELECT * FROM sales_external;
NOTICE:  HEADER means that each one of the data files has a header row
INSERT 0 8
```

–í—ã–ø–æ–ª–Ω–∏–º –∑–∞–ø—Ä–æ—Å: 

```
postgres=# SELECT product_id, SUM(quantity) AS total_sales
postgres-# FROM sales
postgres-# GROUP BY product_id
postgres-# ORDER BY total_sales DESC
postgres-# LIMIT 10;
 product_id | total_sales
------------+-------------
        101 |          13
        103 |           9
        102 |           7
        104 |           1
(4 rows)
```
---
## üçà –£–ø—Ä. - –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞ –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
### –ö–æ–Ω—Ç–µ–∫—Å—Ç:
#### –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω —Å–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö, –≤–∫–ª—é—á–∞—è:
 * –î–∞–Ω–Ω—ã–µ –∏–∑ Data Lake (Amazon S3): –°–æ–±—ã—Ç–∏—è –Ω–∞ —Å–∞–π—Ç–µ (–ø—Ä–æ—Å–º–æ—Ç—Ä—ã —Ç–æ–≤–∞—Ä–æ–≤, –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –∫–æ—Ä–∑–∏–Ω—É, –ø–æ–∫—É–ø–∫–∏) - —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ —Ñ–∞–π–ª–∞—Ö JSON.
 * –î–∞–Ω–Ω—ã–µ –≤ Greenplum:
–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö (ID, email, –¥–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏, –≥–æ—Ä–æ–¥).
–ö–∞—Ç–∞–ª–æ–≥ —Ç–æ–≤–∞—Ä–æ–≤ (ID, –Ω–∞–∑–≤–∞–Ω–∏–µ, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, —Ü–µ–Ω–∞)
#### –¶–µ–ª—å:
–°–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –ø–æ–∫—É–ø–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è.
–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞.
### üçà–†–µ—à–µ–Ω–∏–µ:
1. –°–æ–∑–¥–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –æ —Å–æ–±—ã—Ç–∏—è—Ö:
```
sql
CREATE EXTERNAL TABLE user_events (
    event_time TIMESTAMP,
    user_id INT,
    event_type TEXT,
    product_id INT
)
LOCATION ('s3://my-data-lake/events/')
FORMAT 'JSON'
```
2. –†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (RFM-–∞–Ω–∞–ª–∏–∑):
```
sql
CREATE TABLE user_rfm AS
SELECT 
    u.user_id,
    MAX(e.event_time) AS last_purchase_date,
    COUNT(DISTINCT CASE WHEN e.event_type = 'purchase' THEN e.event_time END) AS purchase_frequency,
    SUM(CASE WHEN e.event_type = 'purchase' THEN 1 ELSE 0 END) AS total_purchases
FROM users u
LEFT JOIN user_events e ON u.user_id = e.user_id
GROUP BY u.user_id;
```
3. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞ K-Means (MADlib):
```
sql
-- –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ K-Means
SELECT madlib.kmeans_train(
    'user_rfm',                -- –¢–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
    'user_rfm_clusters',        -- –í—ã—Ö–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
    'ARRAY[last_purchase_date, purchase_frequency, total_purchases]', -- –°—Ç–æ–ª–±—Ü—ã –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    3                         -- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
);
-- –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö –∫ —Ç–∞–±–ª–∏—Ü–µ user_rfm
ALTER TABLE user_rfm ADD COLUMN cluster_id INT;

UPDATE user_rfm
SET cluster_id = c.cluster_id
FROM user_rfm_clusters c
WHERE user_rfm.user_id = c.user_id;
```
4. –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞:
```
sql
SELECT 
    c.cluster_id,
    p.category,
    COUNT(*) AS total_purchases
FROM user_rfm c
JOIN user_events e ON c.user_id = e.user_id
JOIN products p ON e.product_id = p.product_id
WHERE e.event_type = 'purchase'
GROUP BY c.cluster_id, p.category
ORDER BY c.cluster_id, total_purchases DESC
```
5. –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:

–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –Ω–∞ —Å–∞–π—Ç–µ, –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ email-—Ä–∞—Å—Å—ã–ª–∫–∏ –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å —Ç–∞—Ä–≥–µ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ä–µ–∫–ª–∞–º—É.

---
## üçà –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ Greenplum: –ê–Ω–∞–ª–∏–∑ –∑–∞–∫–∞–∑–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤
#### –®–∞–≥ 1: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã customers
```
postgres=# CREATE TABLE customers (
postgres(#     customer_id INT PRIMARY KEY,
postgres(#     name TEXT,
postgres(#     city TEXT
postgres(# )
postgres-# DISTRIBUTED BY (customer_id);
CREATE TABLE
```
–≠—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å —Å–æ–∑–¥–∞—Å—Ç —Ç–∞–±–ª–∏—Ü—É customers —Å –ø–æ–ª—è–º–∏ customer_id, name –∏ city, –∏—Å–ø–æ–ª—å–∑—É—è customer_id –∫–∞–∫ –∫–ª—é—á —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.

#### –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –∫–ª–∏–µ–Ω—Ç–∞—Ö –∏–∑ CSV-—Ñ–∞–π–ª–∞ [customers.csv](./dataset/customers.csv) (—Å–æ–∑–¥–∞–¥–∏–º –≤–Ω–µ—à–Ω—é—é —Ç–∞–±–ª–∏—Ü—É –∏ —á–µ—Ä–µ–∑ –Ω–µ–µ –∑–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω—É—é):
```
postgres=# CREATE EXTERNAL TABLE ext_customers (
postgres(#     customer_id INT,
postgres(#     name TEXT,
postgres(#     city TEXT
postgres(# )
postgres-# LOCATION ('gpfdist://172.24.0.5:5435/customers.csv')
postgres-# FORMAT 'CSV' (HEADER);
NOTICE:  HEADER means that each one of the data files has a header row
CREATE EXTERNAL TABLE
postgres=# INSERT INTO customers SELECT * FROM ext_customers;
NOTICE:  HEADER means that each one of the data files has a header row
INSERT 0 20
```
#### –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–∫–∞–∑–∞—Ö (hadoop –∏ hive –ø–æ–∫–∞ –Ω–µ –ø—Ä–æ—à–ª–∏ –ø–æ—ç—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∂–µ —á—Ç–æ –∏ –≤ –ø—Ä–æ—à–ª–æ–º –∑–∞–¥–∞–Ω–∏–∏):
```
sql
postgres=# CREATE EXTERNAL TABLE orders (
postgres(#     order_id INT,
postgres(#     customer_id INT,
postgres(#     order_date DATE,
postgres(#     total_amount DECIMAL
postgres(# )
postgres-# LOCATION ('gpfdist://172.24.0.5:5435/orders.csv')
postgres-# FORMAT 'CSV' (HEADER);
NOTICE:  HEADER means that each one of the data files has a header row
CREATE EXTERNAL TABLE
```
#### –®–∞–≥ 4: –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å - —Ç–æ–ø 10 –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–∫–∞–∑–æ–≤
```
sql
postgres=# SELECT
postgres-#     c.customer_id,
postgres-#     c.name,
postgres-#     COUNT(o.order_id) AS total_orders
postgres-# FROM customers c
postgres-# JOIN orders o ON c.customer_id = o.customer_id
postgres-# GROUP BY c.customer_id, c.name
postgres-# ORDER BY total_orders DESC
postgres-# LIMIT 10;
NOTICE:  HEADER means that each one of the data files has a header row
 customer_id |               name                | total_orders
-------------+-----------------------------------+--------------
           1 | –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤             |            5
           3 | –ï–ª–µ–Ω–∞ –ò–≤–∞–Ω–æ–≤–∞         |            4
           5 | –û–ª—å–≥–∞ –í–∞—Å–∏–ª—å–µ–≤–∞     |            3
           2 | –ê–ª–µ–∫—Å–µ–π –°–º–∏—Ä–Ω–æ–≤     |            3
           6 | –°–µ—Ä–≥–µ–π –ú–∏—Ö–∞–π–ª–æ–≤     |            2
           7 | –ê–Ω–Ω–∞ –§–µ–¥–æ—Ä–æ–≤–∞         |            2
           9 | –ù–∞—Ç–∞–ª—å—è –ê–ª–µ–∫—Å–µ–µ–≤–∞ |            2
           8 | –ú–∏—Ö–∞–∏–ª –Ø–∫–æ–≤–ª–µ–≤       |            2
           4 | –î–º–∏—Ç—Ä–∏–π –ö—É–∑–Ω–µ—Ü–æ–≤   |            2
          14 | –ê—Ä—Ç–µ–º –ü–∞–≤–ª–æ–≤           |            1
(10 rows)
```

## üçà –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–¥–∞–Ω–∏—è

1. –û–±—â–∞—è —Å—É–º–º–∞ –∑–∞–∫–∞–∑–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
   
```
sql
postgres=# SELECT
postgres-#     c.customer_id,
postgres-#     c.name,
postgres-#     COUNT(o.order_id) AS total_orders,
postgres-#     SUM(o.total_amount) AS total_spent
postgres-# FROM customers c
postgres-# JOIN orders o ON c.customer_id = o.customer_id
postgres-# GROUP BY c.customer_id, c.name
postgres-# ORDER BY total_spent DESC
postgres-# LIMIT 10;
NOTICE:  HEADER means that each one of the data files has a header row
 customer_id |               name                | total_orders | total_spent
-------------+-----------------------------------+--------------+-------------
           1 | –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤                       |            5 |    67002.00
           3 | –ï–ª–µ–Ω–∞ –ò–≤–∞–Ω–æ–≤–∞     		 |            4 |    46600.00
           5 | –û–ª—å–≥–∞ –í–∞—Å–∏–ª—å–µ–≤–∞  		 |            3 |    37720.75
           2 | –ê–ª–µ–∫—Å–µ–π –°–º–∏—Ä–Ω–æ–≤ 			 |            3 |    33200.75
           9 | –ù–∞—Ç–∞–ª—å—è –ê–ª–µ–∫—Å–µ–µ–≤–∞	         |            2 |    22301.00
           8 | –ú–∏—Ö–∞–∏–ª –Ø–∫–æ–≤–ª–µ–≤                    |            2 |    21700.50
           6 | –°–µ—Ä–≥–µ–π –ú–∏—Ö–∞–π–ª–æ–≤                   |            2 |    20350.75
           4 | –î–º–∏—Ç—Ä–∏–π –ö—É–∑–Ω–µ—Ü–æ–≤                  |            2 |    18850.50
           7 | –ê–Ω–Ω–∞ –§–µ–¥–æ—Ä–æ–≤–∞                     |            2 |    16600.50
          13 | –ú–∞—Ä–∏—è –ö–æ–∑–ª–æ–≤–∞                     |            1 |    12700.25
(10 rows)
```
2. –¢–æ–ø –∫–ª–∏–µ–Ω—Ç–æ–≤ –∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥
```
sql 
postgres=# SELECT
postgres-#     c.customer_id,
postgres-#     c.name,
postgres-#     COUNT(o.order_id) AS total_orders,
postgres-#     SUM(o.total_amount) AS total_spent
postgres-# FROM customers c
postgres-# JOIN orders o ON c.customer_id = o.customer_id
postgres-# WHERE o.order_date BETWEEN '2023-01-01' AND '2023-12-31'
postgres-# GROUP BY c.customer_id, c.name
postgres-# ORDER BY total_spent DESC
postgres-# LIMIT 10;
NOTICE:  HEADER means that each one of the data files has a header row
 customer_id |               name                | total_orders | total_spent
-------------+-----------------------------------+--------------+-------------
           1 | –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤             |            5 |    67002.00
           3 | –ï–ª–µ–Ω–∞ –ò–≤–∞–Ω–æ–≤–∞         |            4 |    46600.00
           5 | –û–ª—å–≥–∞ –í–∞—Å–∏–ª—å–µ–≤–∞     |            3 |    37720.75
           2 | –ê–ª–µ–∫—Å–µ–π –°–º–∏—Ä–Ω–æ–≤     |            3 |    33200.75
           9 | –ù–∞—Ç–∞–ª—å—è –ê–ª–µ–∫—Å–µ–µ–≤–∞ |            2 |    22301.00
           8 | –ú–∏—Ö–∞–∏–ª –Ø–∫–æ–≤–ª–µ–≤       |            2 |    21700.50
           6 | –°–µ—Ä–≥–µ–π –ú–∏—Ö–∞–π–ª–æ–≤     |            2 |    20350.75
           4 | –î–º–∏—Ç—Ä–∏–π –ö—É–∑–Ω–µ—Ü–æ–≤   |            2 |    18850.50
           7 | –ê–Ω–Ω–∞ –§–µ–¥–æ—Ä–æ–≤–∞         |            2 |    16600.50
          13 | –ú–∞—Ä–∏—è –ö–æ–∑–ª–æ–≤–∞         |            1 |    12700.25
(10 rows)
```
3. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–∫–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
```
sql
postgres=# SELECT
postgres-#     customer_id,
postgres-#     name,
postgres-#     total_orders,
postgres-#     total_spent,
postgres-#     RANK() OVER (ORDER BY total_orders DESC) AS orders_rank,
postgres-#     RANK() OVER (ORDER BY total_spent DESC) AS spending_rank
postgres-# FROM (
postgres(#     SELECT
postgres(#         c.customer_id,
postgres(#         c.name,
postgres(#         COUNT(o.order_id) AS total_orders,
postgres(#         SUM(o.total_amount) AS total_spent
postgres(#     FROM customers c
postgres(#     JOIN orders o ON c.customer_id = o.customer_id
postgres(#     GROUP BY c.customer_id, c.name
postgres(# ) AS customer_stats
postgres-# ORDER BY total_orders DESC
postgres-# LIMIT 20;
NOTICE:  HEADER means that each one of the data files has a header row
 customer_id |               name                | total_orders | total_spent | orders_rank | spending_rank
-------------+-----------------------------------+--------------+-------------+-------------+---------------
           1 | –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤             |            5 |    67002.00 |           1 |             1
           3 | –ï–ª–µ–Ω–∞ –ò–≤–∞–Ω–æ–≤–∞         |            4 |    46600.00 |           2 |             2
           5 | –û–ª—å–≥–∞ –í–∞—Å–∏–ª—å–µ–≤–∞     |            3 |    37720.75 |           3 |             3
           2 | –ê–ª–µ–∫—Å–µ–π –°–º–∏—Ä–Ω–æ–≤     |            3 |    33200.75 |           3 |             4
           7 | –ê–Ω–Ω–∞ –§–µ–¥–æ—Ä–æ–≤–∞         |            2 |    16600.50 |           5 |             9
           4 | –î–º–∏—Ç—Ä–∏–π –ö—É–∑–Ω–µ—Ü–æ–≤   |            2 |    18850.50 |           5 |             8
           6 | –°–µ—Ä–≥–µ–π –ú–∏—Ö–∞–π–ª–æ–≤     |            2 |    20350.75 |           5 |             7
           8 | –ú–∏—Ö–∞–∏–ª –Ø–∫–æ–≤–ª–µ–≤       |            2 |    21700.50 |           5 |             6
           9 | –ù–∞—Ç–∞–ª—å—è –ê–ª–µ–∫—Å–µ–µ–≤–∞ |            2 |    22301.00 |           5 |             5
          13 | –ú–∞—Ä–∏—è –ö–æ–∑–ª–æ–≤–∞         |            1 |    12700.25 |          10 |            10
          14 | –ê—Ä—Ç–µ–º –ü–∞–≤–ª–æ–≤           |            1 |    11200.75 |          10 |            11
          11 | –¢–∞—Ç—å—è–Ω–∞ –ù–∏–∫–æ–ª–∞–µ–≤–∞ |            1 |     9400.25 |          10 |            12
          10 | –ê–Ω–¥—Ä–µ–π –ì—Ä–∏–≥–æ—Ä—å–µ–≤   |            1 |     9200.25 |          10 |            13
          12 | –ü–∞–≤–µ–ª –°–µ–º–µ–Ω–æ–≤         |            1 |     7600.00 |          10 |            14
(14 rows)
```
## üçà –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Apache Airflow

–°–æ–∑–¥–∞–¥–∏–º –ø–∞–ø–∫—É airflow, –∏ –∑–∞–≥—Ä—É–∑–∏–º —Ç—É–¥–∞ [docker-compose.yml](dataset/Airflow_docker/docker-compose.yml)  –ø–æ—Å–ª–µ —á–µ–≥–æ –∑–∞–ø—É—Å—Ç–∏–º `docker-compose`

```
C:\Users\79181\docker-greenplum> cd airflow
C:\Users\79181\docker-greenplum\airflow>docker-compose up -d
```
![](./images/7.png)<br>

---

## üçà. - –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è DAG –≤ Airflow

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª my_greenplum_dag.py –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ dags, –Ω–∞–ø—Ä–∏–º–µ—Ä, /path/to/airflow/dags/my_greenplum_dag.py:

```py
from airflow import DAG
from airflow.operators.postgres_operator import PostgresOperator
from airflow.utils.dates import days_ago
# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'retries': 1,
}
# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = DAG('greenplum_data_pipeline', default_args=default_args, schedule_interval='@daily')
# –ó–∞–¥–∞—á–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –≤ Greenplum
create_table = PostgresOperator(
    task_id='create_table',
    postgres_conn_id='greenplum_conn',  # ID —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è Greenplum, –µ–≥–æ –Ω—É–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å
    sql="""
    CREATE TABLE IF NOT EXISTS analytics_data (
        id SERIAL PRIMARY KEY,
        category VARCHAR(50),
        value FLOAT
    );
    """,
    dag=dag,
)
# –ó–∞–¥–∞—á–∞ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É
insert_data = PostgresOperator(
    task_id='insert_data',
    postgres_conn_id='greenplum_conn',
    sql="""
    INSERT INTO analytics_data (category, value) VALUES
    ('category1', 100),
    ('category2', 200),
    ('category3', 300);
    """,
    dag=dag,
)
# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
create_table >> insert_data
```
---
## üçà –£–ø—Ä. - Airflow –∏ Greenplum

–°–æ–∑–¥–∞–¥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:

```
C:\Users\admin\docker-greenplum\airflow>docker exec -it airflow-airflow-webserver-1 airflow users create --username nata --password nata --firstname natalia --lastname brykovskaya --role Admin --email br.nat.vas@gmail.com
[2025-04-13T20:21:20+03:00] {cli_action_loggers.py:105} WARNING - Failed to log action with (psycopg2.errors.UndefinedTable) relation "log" does not exist
LINE 1: INSERT INTO log (dttm, dag_id, task_id, event, execution_dat...
                    ^

[SQL: INSERT INTO log (dttm, dag_id, task_id, event, execution_date, owner, extra) VALUES (%(dttm)s, %(dag_id)s, %(task_id)s, %(event)s, %(execution_date)s, %(owner)s, %(extra)s) RETURNING log.id]
[parameters: {'dttm': datetime.datetime(2025, 4, 13, 20, 54, 5, 146386, tzinfo=Timezone('UTC')), 'dag_id': None, 'task_id': None, 'event': 'cli_users_create', 'execution_date': None, 'owner': 'airflow', 'extra': '{"host_name": "5556baddb669", "full_command": "[\'/home/airflow/.local/bin/airflow\', \'users\', \'create\', \'--username\', \'nata\', \'--password\', \'********\', \'--firstname\', \'natalia\', \'--lastname\', \'brykovskaya\', \'--role\', \'Admin\', \'--email\', \'br.nat.vas@gmail.com\']"}'}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2025-04-13T20:21:22+03:00] {manager.py:763} WARNING - No user yet created, use flask fab command to do it.
[2025-04-13T20:21:22+03:00] {manager.py:512} WARNING - Refused to delete permission view, assoc with role exists DAG Runs.can_create User
[2025-04-13T20:21:22+03:00] {manager.py:214} INFO - Added user nata
User "nata" created with role "Admin"
```
–ù–∞—Å—Ç—Ä–æ–∏–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –º–µ–∂–¥—É AirFlow –∏ GreenPLum (–ó–∞–π–¥–µ–º –ø–æ –∞–¥—Ä–µ—Å—É http://localhost:8081/, –ø–æ–¥ –Ω–æ–≤—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º, –¥–∞–ª–µ–µ –∑–∞–π–¥–µ–º Admin - connections)
![alt text](./images/image3.png)

–ü—Ä–æ–≤–µ—Ä–∏–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞:
```
bash
airflow@1ba487c959c6:/opt/airflow$ airflow connections get greenplum_conn
+----+------------+-------------+-----------+-----------+-----------+---------+----------+------+-----------+--------------+----------+--------------------------------+
| id |  conn_id   |  conn_type  | descript  |   host    |  schema   |  login  | password | port | is_encr   | is_extra_enc | extra_de |             get_uri            |
|    |            |             |   ion     |           |           |         |          |      | ypted     |    rypted    |   json   |                                |
+----+------------+-------------+-----------+-----------+-----------+---------+----------+------+-----------+--------------+----------+--------------------------------+
| 3  |greenplum_co|   postgres  |   None    | db_master | postgres  | gpadmin |   None   | 5432 |   False   |    False     |    {}    | postgres://gpadmin@db_master:54|
|    |     nn     |             |           |           |           |         |          |      |           |              |          | 32/postgres                    |
+----+------------+-------------+-----------+-----------+-----------+---------+----------+------+-----------+--------------+----------+--------------------------------+
```

–ü–æ–º–µ—Å—Ç–∏–º –Ω–∞—à [DAG-file](./dataset/greenplum_etl.py) –≤ –ø–∞–ø–∫—É airflow/dags –∏ –ø—Ä–æ–≤–µ—Ä–∏–º –∑–∞–≥—Ä—É–∑–∫—É:

```
bash
airflow@1ba487c959c6:/opt/airflow$ airflow dags list
dag_id         | filepath         | owner   | paused
===============+==================+=========+=======
greenplum_etl  | greenplum_etl.py | airflow | None
test_greenplum | test_dag.py      | airflow | None
```
---
## üçà  –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –°–æ–∑–¥–∞–Ω–∏–µ DAG —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏

–°–æ–∑–¥–∞–¥–∏–º [DAG](./dataset/DAG/DAG1.py):

```
py
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'retries': 1,
}
dag = DAG('complex_dag', default_args=default_args, schedule_interval='@daily')
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
download_data = BashOperator(
    task_id='download_data',
    bash_command='hdfs dfs -get hdfs://namenode:9000/path/to/input.csv /path/to/local/input.csv',
    dag=dag,
)
# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
process_data = BashOperator(
    task_id='process_data',
    bash_command='python /path/to/your_script.py',
    dag=dag,
)
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
save_data = BashOperator(
    task_id='save_data',
    bash_command='hdfs dfs -put /path/to/local/output.csv hdfs://namenode:9000/path/to/output.csv',
    dag=dag,
)
download_data >> process_data >> save_data
```
---
## üçà –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ PythonOperator –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ Python

–°–æ–∑–¥–∞–¥–∏–º [DAG](./dataset/DAG/DAG2.py):

```
py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
def calculate_and_write():
    a = 10
    b = 5
    sum_result = a + b
    diff_result = a - b
    
    with open('/path/to/results.txt', 'w') as f:
        f.write(f'Sum: {sum_result}\n')
        f.write(f'Difference: {diff_result}\n')
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
}
dag = DAG('python_operator_dag', default_args=default_args, schedule_interval='@daily')
calculate_task = PythonOperator(
    task_id='calculate_and_write',
    python_callable=calculate_and_write,
    dag=dag,
)
```
---
## üçà –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ XCom –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏

–°–æ–∑–∞–¥–∏–º [DAG-file](./dataset/DAG/DAG3.py):

```
py
import random
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
def generate_random_number(**kwargs):
    random_number = random.randint(1, 100)
    kwargs['ti'].xcom_push(key='random_number', value=random_number)
def multiply_random_number(**kwargs):
    ti = kwargs['ti']
    random_number = ti.xcom_pull(key='random_number', task_ids='generate_random_number')
    result = random_number * 2
    print(f'Multiplied value: {result}')
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
}
dag = DAG('xcom_dag', default_args=default_args, schedule_interval='@daily')
generate_task = PythonOperator(
    task_id='generate_random_number',
    python_callable=generate_random_number,
    provide_context=True,
    dag=dag,
)
multiply_task = PythonOperator(
    task_id='multiply_random_number',
    python_callable=multiply_random_number,provide_context=True,
    dag=dag,
)
generate_task >> multiply_task
```
---

## üçà –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –°–æ–∑–¥–∞–Ω–∏–µ DAG —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤

–°–æ–∑–¥–∞–¥–∏–º [DAG-file](./dataset/DAG/DAG4.py)):

```
py
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.utils.dates import days_ago
def process_data():
    # –í–∞—à–∞ –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    pass
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
}
dag = DAG('mixed_operators_dag', default_args=default_args, schedule_interval='@daily')
download_data = BashOperator(
    task_id='download_data',
    bash_command='hdfs dfs -get hdfs://namenode:9000/path/to/input.csv /path/to/local/input.csv',
    dag=dag,
)
process_data_task = PythonOperator(
    task_id='process_data',
    python_callable=process_data,
    dag=dag,
)
save_to_postgres = PostgresOperator(
    task_id='save_to_postgres',
    postgres_conn_id='postgres_default',  # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ
    sql='INSERT INTO your_table (col1, col2) VALUES (value1, value2);',
    dag=dag,
)
download_data >> process_data_task >> save_to_postgres
```
---
## üçà  –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è

–°–æ–∑–¥–∞–¥–∏–º[DAG-file](./dataset/DAG/DAG5.py):

```
py
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.operators.email import EmailOperator
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
def failing_task():
    raise Exception("This task is intended to fail.")
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email': ['your_email@example.com'],  # –£–∫–∞–∂–∏—Ç–µ —Å–≤–æ–π –∞–¥—Ä–µ—Å —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç—ã
}
dag = DAG('notification_dag', default_args=default_args, schedule_interval='@daily')
start = DummyOperator(task_id='start', dag=dag)
failing_task = PythonOperator(
    task_id='failing_task',
    python_callable=failing_task,
    dag=dag,
)
send_email = EmailOperator(
    task_id='send_email',
    to='your_email@example.com',
    subject='Airflow Task Failed',
    html_content='The task has failed!',
    dag=dag,
)
start >> failing_task >> send_email
```